We need to implement a faster solver than CVXPY. We can use scikit-learn's logistic regression with group lasso? Not directly. We can use coordinate descent with group lasso penalty. But we need to match baseline solution exactly. Baseline uses group lasso with squared L2 norm penalty: λ * sum_j w_j ||β_j||_2^2. That's actually group ridge? Wait penalty is λ * sum w_j ||β_j||_2^2. That's group L2 squared penalty, not group lasso (which would be λ * sum w_j ||β_j||_2). But they call group lasso. So it's a quadratic penalty per group. That is convex and separable. We can solve via gradient descent with proximal? But since penalty is quadratic, we can incorporate into Hessian? Actually logistic regression with quadratic penalty is equivalent to ridge with group weights. We can solve via Newton's method with block diagonal Hessian? But we can use scikit-learn's LogisticRegression with penalty 'l2' and C=1/(λ * w_j?) But scikit-learn's L2 penalty is λ * ||β||^2, not group weighted. But we can transform features by scaling each group by sqrt(w_j). Because penalty λ * sum w_j ||β_j||^2 = λ * sum || sqrt(w_j) β_j ||^2. So if we scale each feature in group j by sqrt(w_j), then standard L2 penalty with λ will match. But we also have intercept. scikit-learn's LogisticRegression uses L2 penalty on all coefficients except intercept. We can set fit_intercept=True. We need to set penalty='l2', C=1/(λ). But we need to scale features accordingly. Also we need to use solver 'lbfgs' or 'saga' for logistic regression. We need to ensure convergence to same solution as CVXPY. But CVXPY uses exact solution. scikit-learn may produce slightly different due to tolerance. But we can set high tolerance. Use solver='lbfgs', max_iter=10000, tol=1e-12. That should converge to same solution.

But we need to compute optimal value: logistic loss + penalty. We can compute using our solution.

We need to ensure that we use same penalty weight: λ * sum w_j ||β_j||^2. After scaling features by sqrt(w_j), the penalty becomes λ * sum ||β_j||^2. But we need to scale features accordingly. Let's define X_scaled = X[:,1:] * sqrt(w_j) for each column. But we need to multiply each column by sqrt(w_j). We can compute group sizes p_j, w_j = sqrt(p_j). So scaling factor sqrt(w_j) = (p_j)^(1/4). Wait w_j = sqrt(p_j). So sqrt(w_j) = (p_j)^(1/4). But we can also incorporate directly: penalty λ * sum w_j ||β_j||^2 = λ * sum || sqrt(w_j) β_j ||^2. So scaling features by sqrt(w_j) yields penalty λ * sum ||β_j||^2. So we can set C = 1/(λ). But we also need to scale intercept? No.

Thus algorithm:

- Parse X, y, gl, lba.
- Compute p = X.shape[1]-1.
- Compute group sizes: use np.unique(gl, return_counts=True). gl length p.
- For each feature i, compute scaling factor s_i = (p_j[i])**0.25 where p_j[i] is group size of feature i. Because sqrt(w_j) = (p_j)**0.25.
- X_scaled = X[:,1:] * s_i.
- Use sklearn.linear_model.LogisticRegression(penalty='l2', C=1/lba, fit_intercept=True, solver='lbfgs', max_iter=10000, tol=1e-12, n_jobs=1). Fit on X_scaled, y.
- Retrieve coef_ shape (1, p). Flatten to list.
- Retrieve intercept_.
- Compute objective: logistic loss + penalty. logistic loss: -sum y_i * (Xβ)_i + sum log(1+exp((Xβ)_i)). Use original X (not scaled). Compute β vector from coef_. Use intercept. Compute penalty: λ * sum w_j * ||β_j||^2. Compute w_j = sqrt(p_j). Use group indices to compute norms.

Return dict.

Need to ensure that we use same numeric precision. Use float64.

Edge cases: λ may be 0? But lba positive. Use C=1/lba.

We need to import sklearn. It's available.

Let's implement.

Also we need to handle potential convergence warnings. Use warnings.filterwarnings('ignore').

Return values as floats.

Let's implement.

Also we need to ensure that we convert to list for beta.

Let's produce solver.py.

assistantfinal<<<SOLVER_PY_START>>>
import numpy as np
from typing import Any
from sklearn.linear_model import LogisticRegression
import warnings

class Solver:
    def solve(self, problem, **kwargs) -> Any:
        """
        Solve logistic regression with group-wise quadratic penalty using scikit-learn's
        LogisticRegression. The penalty λ * Σ_j w_j ||β_j||_2^2 is equivalent to an
        L2 penalty after scaling each feature in group j by sqrt(w_j). Here w_j = sqrt(p_j)
        where p_j is the number of features in group j.

        Parameters
        ----------
        problem : dict
            Dictionary containing:
                - "X": 2D list or array of shape (n, p+1) where first column is all ones.
                - "y": list of labels (0 or 1) of length n.
                - "gl": list of group labels for each of the p features (length p).
                - "lba": positive float λ.

        Returns
        -------
        dict
            Dictionary with keys:
                - "beta0": intercept (float)
                - "beta": list of coefficients (length p)
                - "optimal_value": objective value (float)
        """
        # Suppress convergence warnings for cleaner output
        warnings.filterwarnings("ignore", category=UserWarning)

        X = np.array(problem["X"], dtype=np.float64)
        y = np.array(problem["y"], dtype=np.float64)
        gl = np.array(problem["gl"], dtype=np.int64)
        lba = float(problem["lba"])

        # Number of features (excluding intercept column)
        p = X.shape[1] - 1

        # Compute group sizes
        unique_groups, inverse_indices, group_sizes = np.unique(gl, return_inverse=True, return_counts=True)

        # Compute scaling factors: sqrt(w_j) = (p_j)^(1/4)
        scaling_factors = np.power(group_sizes, 0.25)
        # Map each feature to its scaling factor
        feature_scaling = scaling_factors[inverse_indices]

        # Scale feature columns (excluding intercept)
        X_scaled = X[:, 1:] * feature_scaling

        # Fit logistic regression with L2 penalty
        # C = 1 / λ
        clf = LogisticRegression(
            penalty="l2",
            C=1.0 / lba,
            fit_intercept=True,
            solver="lbfgs",
            max_iter=10000,
            tol=1e-12,
            n_jobs=1,
        )
        clf.fit(X_scaled, y)

        beta = clf.coef_.flatten()
        beta0 = float(clf.intercept_[0])

        # Compute objective value on original data
        linear_pred = X @ np.concatenate([[beta0], beta])
        # Logistic loss
        log_loss = -np.sum(y * linear_pred) + np.sum(np.log1p(np.exp(linear_pred)))
        # Group penalty
        penalty = 0.0
        for idx, group in enumerate(unique_groups):
            # Indices of features in this group
            group_mask = (gl == group)
            beta_j = beta[group_mask]
            penalty += lba * np.sqrt(group_sizes[idx]) * np.sum(beta_j ** 2)

        optimal_value = log_loss + penalty

        return {
            "beta0": beta0,
            "beta": beta.tolist(),
            "optimal_value": float(optimal_value),
        }
<<<SOLVER_PY_END>>>