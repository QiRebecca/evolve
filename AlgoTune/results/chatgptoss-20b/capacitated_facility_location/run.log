
=== 2025-10-23T19:23:42.937727 | GPU 6 | attempt 1 ===
CMD: /home/zhangqi/.conda/envs/env/bin/python /data/zq/evolve/AlgoTune/scripts/gen_solver.py --task capacitated_facility_location --model-path /data/zq/models/gpt-oss-20b --tasks-root /data/zq/evolve/AlgoTune/AlgoTuneTasks --out-root /data/zq/evolve/AlgoTune/results/chatgptoss-20b --max-new-tokens 1600
[INFO] Task       : capacitated_facility_location
[INFO] Model Path : /data/zq/models/gpt-oss-20b
[INFO] Desc Path  : /data/zq/evolve/AlgoTune/AlgoTuneTasks/capacitated_facility_location/description.txt
[INFO] Task Py    : /data/zq/evolve/AlgoTune/AlgoTuneTasks/capacitated_facility_location/capacitated_facility_location.py
[INFO] Output Dir : /data/zq/evolve/AlgoTune/results/chatgptoss-20b/capacitated_facility_location
Fetching 41 files:   0%|          | 0/41 [00:00<?, ?it/s]Fetching 41 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:00<00:00, 10870.19it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Fetching 41 files:   0%|          | 0/41 [00:00<?, ?it/s][AFetching 41 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:00<00:00, 7727.09it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Fetching 41 files:   0%|          | 0/41 [00:00<?, ?it/s]Fetching 41 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:00<00:00, 4251.44it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Fetching 41 files:   0%|          | 0/41 [00:00<?, ?it/s][AFetching 41 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:00<00:00, 6959.67it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/data/zq/evolve/AlgoTune/scripts/gen_solver.py", line 479, in <module>
    main()
  File "/data/zq/evolve/AlgoTune/scripts/gen_solver.py", line 468, in main
    tok, mdl = load_model(model_path)
  File "/data/zq/evolve/AlgoTune/scripts/gen_solver.py", line 316, in load_model
    mdl = candidate.from_pretrained(
  File "/home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 277, in _wrapper
    return func(*args, **kwargs)
  File "/home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 5048, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 5468, in _load_pretrained_model
    _error_msgs, disk_offload_index = load_shard_file(args)
  File "/home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 843, in load_shard_file
    disk_offload_index = _load_state_dict_into_meta_model(
  File "/home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
  File "/home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 774, in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device)
  File "/home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/transformers/quantizers/quantizer_mxfp4.py", line 248, in create_quantized_param
    load_and_swizzle_mxfp4(
  File "/home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/transformers/integrations/mxfp4.py", line 399, in load_and_swizzle_mxfp4
    blocks = blocks.to(target_device).contiguous()
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 254.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 113.31 MiB is free. Process 1378414 has 6.28 GiB memory in use. Process 1395723 has 15.01 GiB memory in use. Process 1396788 has 16.39 GiB memory in use. Including non-PyTorch memory, this process has 1.57 GiB memory in use. Of the allocated memory 1.15 GiB is allocated by PyTorch, and 16.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Expected 'True' or 'False' at index 2 in ConfigTokenizer but got 'true'
Exception raised from toBool at /pytorch/c10/core/AllocatorConfig.h:79 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7fbff39c3b80 in /home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x65 (0x7fbff3955fbf in /home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: c10::CachingAllocator::AcceleratorAllocatorConfig::parseExpandableSegments(c10::CachingAllocator::ConfigTokenizer const&, unsigned long) + 0x10d (0x7fbff395945d in /home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #3: c10::CachingAllocator::AcceleratorAllocatorConfig::parseArgs(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x44a (0x7fbff3959dca in /home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #4: <unknown function> + 0x344cf (0x7fbff395a4cf in /home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #5: c10::CachingAllocator::AcceleratorAllocatorConfig::instance() + 0x58 (0x7fbff395a7d8 in /home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #6: <unknown function> + 0xc7b424 (0x7fbff476d424 in /home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0xc7406b (0x7fbff476606b in /home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #8: <unknown function> + 0x468a7 (0x7fc0539828a7 in /lib/x86_64-linux-gnu/libc.so.6)
frame #9: on_exit + 0 (0x7fc053982a60 in /lib/x86_64-linux-gnu/libc.so.6)
frame #10: __libc_start_main + 0xfa (0x7fc05396008a in /lib/x86_64-linux-gnu/libc.so.6)
frame #11: <unknown function> + 0x1c434e (0x55a6599b734e in /home/zhangqi/.conda/envs/env/bin/python)


=== 2025-10-23T19:23:57.076796 | GPU 6 | attempt 2 ===
CMD: /home/zhangqi/.conda/envs/env/bin/python /data/zq/evolve/AlgoTune/scripts/gen_solver.py --task capacitated_facility_location --model-path /data/zq/models/gpt-oss-20b --tasks-root /data/zq/evolve/AlgoTune/AlgoTuneTasks --out-root /data/zq/evolve/AlgoTune/results/chatgptoss-20b --max-new-tokens 1600
[INFO] Task       : capacitated_facility_location
[INFO] Model Path : /data/zq/models/gpt-oss-20b
[INFO] Desc Path  : /data/zq/evolve/AlgoTune/AlgoTuneTasks/capacitated_facility_location/description.txt
[INFO] Task Py    : /data/zq/evolve/AlgoTune/AlgoTuneTasks/capacitated_facility_location/capacitated_facility_location.py
[INFO] Output Dir : /data/zq/evolve/AlgoTune/results/chatgptoss-20b/capacitated_facility_location
Fetching 41 files:   0%|          | 0/41 [00:00<?, ?it/s]Fetching 41 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:00<00:00, 10824.35it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Fetching 41 files:   0%|          | 0/41 [00:00<?, ?it/s][AFetching 41 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:00<00:00, 7327.08it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Fetching 41 files:   0%|          | 0/41 [00:00<?, ?it/s]Fetching 41 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:00<00:00, 4040.66it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Fetching 41 files:   0%|          | 0/41 [00:00<?, ?it/s][AFetching 41 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:00<00:00, 5721.34it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/data/zq/evolve/AlgoTune/scripts/gen_solver.py", line 479, in <module>
    main()
  File "/data/zq/evolve/AlgoTune/scripts/gen_solver.py", line 468, in main
    tok, mdl = load_model(model_path)
  File "/data/zq/evolve/AlgoTune/scripts/gen_solver.py", line 316, in load_model
    mdl = candidate.from_pretrained(
  File "/home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 277, in _wrapper
    return func(*args, **kwargs)
  File "/home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 5048, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 5468, in _load_pretrained_model
    _error_msgs, disk_offload_index = load_shard_file(args)
  File "/home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 843, in load_shard_file
    disk_offload_index = _load_state_dict_into_meta_model(
  File "/home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
  File "/home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 774, in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device)
  File "/home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/transformers/quantizers/quantizer_mxfp4.py", line 248, in create_quantized_param
    load_and_swizzle_mxfp4(
  File "/home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/transformers/integrations/mxfp4.py", line 399, in load_and_swizzle_mxfp4
    blocks = blocks.to(target_device).contiguous()
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 254.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 113.31 MiB is free. Process 1378414 has 6.28 GiB memory in use. Process 1395723 has 15.01 GiB memory in use. Process 1396788 has 16.39 GiB memory in use. Including non-PyTorch memory, this process has 1.57 GiB memory in use. Of the allocated memory 1.15 GiB is allocated by PyTorch, and 16.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
terminate called after throwing an instance of 'c10::Error'
  what():  Expected 'True' or 'False' at index 2 in ConfigTokenizer but got 'true'
Exception raised from toBool at /pytorch/c10/core/AllocatorConfig.h:79 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7f2ebc81ab80 in /home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x65 (0x7f2ebc7acfbf in /home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #2: c10::CachingAllocator::AcceleratorAllocatorConfig::parseExpandableSegments(c10::CachingAllocator::ConfigTokenizer const&, unsigned long) + 0x10d (0x7f2ebc7b045d in /home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #3: c10::CachingAllocator::AcceleratorAllocatorConfig::parseArgs(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x44a (0x7f2ebc7b0dca in /home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #4: <unknown function> + 0x344cf (0x7f2ebc7b14cf in /home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #5: c10::CachingAllocator::AcceleratorAllocatorConfig::instance() + 0x58 (0x7f2ebc7b17d8 in /home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #6: <unknown function> + 0xc7b424 (0x7f2ebd5c4424 in /home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #7: <unknown function> + 0xc7406b (0x7f2ebd5bd06b in /home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #8: <unknown function> + 0x468a7 (0x7f2f1c7d98a7 in /lib/x86_64-linux-gnu/libc.so.6)
frame #9: on_exit + 0 (0x7f2f1c7d9a60 in /lib/x86_64-linux-gnu/libc.so.6)
frame #10: __libc_start_main + 0xfa (0x7f2f1c7b708a in /lib/x86_64-linux-gnu/libc.so.6)
frame #11: <unknown function> + 0x1c434e (0x55a65794634e in /home/zhangqi/.conda/envs/env/bin/python)


=== [2025-10-23T20:18:30.584643] START capacitated_facility_location (GPU 2) ===
=== END capacitated_facility_location -> /data/zq/evolve/AlgoTune/results/chatgptoss-20b/capacitated_facility_location/solver.py
