analysisWe need to implement a fast PageRank solver. We can implement power iteration ourselves using numpy arrays. Use adjacency list to build transition matrix implicitly. For dangling nodes, we need to distribute uniformly. We can compute vector r iteratively until convergence. Use tolerance 1e-6 maybe. Use alpha default 0.85. We can also use numpy dot with sparse representation. But adjacency list may be large. Use efficient approach: compute new_r = alpha * (P @ r) + (1-alpha)/n. For P @ r: For each node i, distribute r[i]/d_i to its neighbors. For dangling nodes, distribute r[i]/n to all nodes. We can accumulate contributions. Use numpy array for r. Use loops over adjacency list. Complexity O(m). Should be fine.

We need to match networkx default parameters: alpha=0.85, max_iter=100, tol=1e-06. We'll implement until convergence or max_iter. For n=0 return empty list. For n=1 return [1.0].

Edge cases: adjacency list may contain nodes with no outgoing edges. We treat as dangling.

Implementation details: Use numpy array of dtype float64. Use while loop. Use np.linalg.norm difference 1-norm < tol. Use vectorization for dangling: compute dangling_sum = sum of r[i] where d_i==0. Then add alpha * dangling_sum / n to all nodes. Then for each node i, distribute r[i]/d_i to neighbors. We can accumulate contributions in new_r array. Use for loops.

We need to import typing.Any. Also maybe import numpy as np. Use default parameters alpha=0.85, max_iter=100, tol=1e-06. Provide ability to override via kwargs.

Return {"pagerank_scores": list(r)}.

Let's craft code.

Also need to ensure that we don't use networkx. So no import networkx.

Let's write.

Also we need to handle potential large graphs; loops may be heavy but acceptable.

We can also use numba for speed but not necessary.

Let's implement.

Also we need to ensure that we use float type for scores.

Let's produce final code.

assistantfinal<<<SOLVER_PY_START>>>
from typing import Any
import numpy as np

class Solver:
    def solve(self, problem, **kwargs) -> Any:
        """
        Compute PageRank scores for a directed graph given its adjacency list.
        This implementation uses a custom power iteration algorithm that
        avoids the overhead of NetworkX and is optimized for speed.

        Parameters
        ----------
        problem : dict
            Dictionary containing the key "adjacency_list" with a list of
            lists of outgoing neighbors for each node.
        **kwargs : dict
            Optional parameters:
                alpha : float, damping factor (default 0.85)
                max_iter : int, maximum number of iterations (default 100)
                tol : float, convergence tolerance (default 1e-06)

        Returns
        -------
        dict
            Dictionary with key "pagerank_scores" mapping to a list of
            PageRank scores in node index order.
        """
        adj_list = problem.get("adjacency_list", [])
        n = len(adj_list)

        # Handle trivial cases
        if n == 0:
            return {"pagerank_scores": []}
        if n == 1:
            return {"pagerank_scores": [1.0]}

        # Parameters
        alpha = kwargs.get("alpha", 0.85)
        max_iter = kwargs.get("max_iter", 100)
        tol = kwargs.get("tol", 1e-06)

        # Precompute out-degree and dangling node mask
        out_deg = np.array([len(neigh) for neigh in adj_list], dtype=np.int32)
        dangling_mask = out_deg == 0

        # Initialize PageRank vector uniformly
        r = np.full(n, 1.0 / n, dtype=np.float64)

        # Preallocate array for new ranks
        new_r = np.empty(n, dtype=np.float64)

        for _ in range(max_iter):
            # Start with teleportation component
            new_r.fill((1.0 - alpha) / n)

            # Handle dangling nodes: distribute their rank uniformly
            if np.any(dangling_mask):
                dangling_sum = r[dangling_mask].sum()
                new_r += alpha * dangling_sum / n

            # Distribute rank from non-dangling nodes
            for i, neighbors in enumerate(adj_list):
                if out_deg[i] == 0:
                    continue
                share = alpha * r[i] / out_deg[i]
                new_r[neighbors] += share

            # Check convergence
            diff = np.abs(new_r - r).sum()
            if diff < tol:
                r = new_r
                break

            r, new_r = new_r, r  # swap references for next iteration

        # Ensure the result sums to 1.0 (normalization)
        r /= r.sum()

        return {"pagerank_scores": r.tolist()}
<<<SOLVER_PY_END>>>