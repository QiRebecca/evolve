"""Unified evaluation entry point for AlgoTune solvers.

This module exposes an ``evaluate`` function compatible with OpenEvolve's
``Evaluator`` while remaining convenient for direct command line usage.
It loads datasets generated by AlgoTune tasks, runs a candidate ``solver.py``
implementation against them, and reports aggregate metrics such as mean
speedup and accuracy using AlgoTune's native evaluation pipeline.

Under the hood the adapter resolves task configuration, instantiates the
original AlgoTune :class:`Task` objects, and delegates execution to
``AlgoTuner.utils.evaluator.main.evaluate_code_on_dataset``—the same function
used by AlgoTune's internal evaluator.  The candidate solver is dynamically
imported and bound onto a task instance so the native evaluator can execute it
without modifications.

The evaluator expects a task name via configuration (either the ``config``
argument or environment variable ``ALGO_TUNE_TASK``) and a data directory
containing the generated ``*.jsonl`` files (defaults to
``/data/zq/evolve/AlgoTune/data``).  Results are returned as an
``EvaluationResult`` so that OpenEvolve can store both metrics and optional
artifacts.
"""

from __future__ import annotations

import argparse
import importlib
import importlib.util
import json
import logging
import os
import sys
from pathlib import Path
from typing import Any, Callable, Dict, Iterable, List, Optional

from openevolve.evaluation_result import EvaluationResult

# Ensure the AlgoTune repository (this file's parent) is importable
REPO_ROOT = Path(__file__).resolve().parent
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from AlgoTuneTasks.base import TASK_REGISTRY, Task  # type: ignore  # noqa: E402

# Configure a module-level logger
LOGGER = logging.getLogger("algotune.evaluate")
if not LOGGER.handlers:
    logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")


DEFAULT_DATA_DIR = Path("/data/zq/evolve/AlgoTune/data")
DEFAULT_GENERATION_FILE = Path("/data/zq/evolve/reports/generation.json")


def _parse_int(value: Optional[str]) -> Optional[int]:
    if value is None or value == "":
        return None
    try:
        return int(value)
    except ValueError:
        LOGGER.warning("Unable to parse integer from %s", value)
        return None


def _parse_float(value: Optional[str]) -> Optional[float]:
    if value is None or value == "":
        return None
    try:
        return float(value)
    except ValueError:
        LOGGER.warning("Unable to parse float from %s", value)
        return None


def _resolve_config(overrides: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """Resolve configuration from overrides and environment variables."""

    config: Dict[str, Any] = {
        "task_name": os.environ.get("ALGO_TUNE_TASK"),
        "data_dir": os.environ.get("ALGO_TUNE_DATA_DIR"),
        "split": os.environ.get("ALGO_TUNE_SPLIT", "train"),  # Changed default to train for OpenEvolve
        "max_samples": _parse_int(os.environ.get("ALGO_TUNE_MAX_SAMPLES")),
        "timeout_multiplier": _parse_float(os.environ.get("ALGO_TUNE_TIMEOUT_MULTIPLIER")),
        "min_timeout_seconds": _parse_float(os.environ.get("ALGO_TUNE_MIN_TIMEOUT")),
        "max_timeout_seconds": _parse_float(os.environ.get("ALGO_TUNE_MAX_TIMEOUT")),
        "num_runs": _parse_int(os.environ.get("ALGO_TUNE_NUM_RUNS")),
        "warmup_runs": _parse_int(os.environ.get("ALGO_TUNE_WARMUP_RUNS")),
        "test_mode": os.environ.get("ALGO_TUNE_TEST_MODE", "0") == "1",
        "generation_file": os.environ.get("ALGO_TUNE_GENERATION_FILE"),
    }

    if overrides:
        for key, value in overrides.items():
            if value is not None:
                config[key] = value

    # Task name is mandatory
    if not config.get("task_name"):
        raise ValueError(
            "Task name not provided. Set ALGO_TUNE_TASK env var or pass task_name in config."
        )

    data_dir = config.get("data_dir")
    if data_dir:
        config["data_dir"] = Path(data_dir)
    else:
        config["data_dir"] = DEFAULT_DATA_DIR

    generation_file = config.get("generation_file")
    if generation_file:
        generation_path = Path(generation_file)
        config["generation_file"] = generation_path
    elif DEFAULT_GENERATION_FILE.exists():
        config["generation_file"] = DEFAULT_GENERATION_FILE
    else:
        config["generation_file"] = None

    if config["split"] not in {"train", "test"}:
        raise ValueError("split must be 'train' or 'test'")

    return config


def _import_task_module(task_name: str) -> None:
    """Import the AlgoTune task module to populate TASK_REGISTRY."""

    if task_name in TASK_REGISTRY:
        return

    module_name = f"AlgoTuneTasks.{task_name}.{task_name}"
    LOGGER.info("Importing task module %s", module_name)
    importlib.import_module(module_name)


def _load_task_class(task_name: str) -> type[Task]:
    _import_task_module(task_name)
    if task_name not in TASK_REGISTRY:
        available = ", ".join(sorted(TASK_REGISTRY.keys()))
        raise ValueError(f"Task '{task_name}' not found in TASK_REGISTRY. Available: {available}")
    task_cls = TASK_REGISTRY[task_name]
    if not issubclass(task_cls, Task):
        raise TypeError(f"Registered task '{task_name}' is not a Task subclass")
    return task_cls


def _safe_json(data: Any) -> str:
    """Serialize arbitrary data to JSON, converting non-serializable values."""

    def convert(obj: Any) -> Any:
        if isinstance(obj, (str, int, float, bool)) or obj is None:
            return obj
        if isinstance(obj, Path):
            return str(obj)
        if isinstance(obj, dict):
            return {str(k): convert(v) for k, v in obj.items()}
        if isinstance(obj, (list, tuple, set)):
            return [convert(v) for v in obj]
        if hasattr(obj, "_asdict"):
            return convert(obj._asdict())
        if hasattr(obj, "__dict__"):
            try:
                return convert(vars(obj))
            except Exception:  # pragma: no cover - fallback
                pass
        return repr(obj)

    return json.dumps(convert(data), indent=2, ensure_ascii=False)


def _load_solver_callable(
    program_path: str,
    task_class: type[Task],
    candidate_task: Task,
) -> Callable[[Any], Any]:
    """Load a solver callable from ``program_path``.

    Supports multiple patterns:
      * A top-level ``solve(problem)`` function
      * A ``run_solver(problem)`` function
      * A ``Solver`` class with a ``solve`` method
      * A class matching the task name that implements ``solve``
    """

    spec = importlib.util.spec_from_file_location("candidate_solver", program_path)
    if spec is None or spec.loader is None:
        raise ImportError(f"Could not load solver module from {program_path}")

    module = importlib.util.module_from_spec(spec)
    sys.modules[spec.name] = module
    spec.loader.exec_module(module)

    candidates: List[Callable[..., Any]] = []

    for attr_name in ("solve", "run_solver"):
        candidate = getattr(module, attr_name, None)
        if callable(candidate):
            candidates.append(candidate)

    solver_cls = getattr(module, "Solver", None)
    if solver_cls is not None:
        try:
            instance = solver_cls()
            if hasattr(instance, "solve") and callable(instance.solve):
                candidates.append(instance.solve)
        except Exception as exc:  # pragma: no cover - defensive
            LOGGER.warning("Failed to instantiate Solver class: %s", exc)

    task_cls_name = task_class.__name__
    module_task_cls = getattr(module, task_cls_name, None)
    if module_task_cls is not None:
        try:
            instance = module_task_cls()
            if hasattr(instance, "solve") and callable(instance.solve):
                candidates.append(instance.solve)
        except Exception as exc:  # pragma: no cover - defensive
            LOGGER.warning("Failed to instantiate %s from solver module: %s", task_cls_name, exc)

    if not candidates:
        raise AttributeError(
            "Solver module must define one of: solve(), run_solver(), Solver.solve(), "
            f"or {task_cls_name}.solve()"
        )

    solver_callable = candidates[0]

    def _wrapped_solver(problem: Any) -> Any:
        try:
            result = solver_callable(problem)
        except TypeError as exc:
            # Some solvers expect the task instance as an additional argument
            if "missing" in str(exc) and "positional argument" in str(exc):
                result = solver_callable(problem, candidate_task)
            else:
                raise
        
        # Convert numpy arrays to lists for validation compatibility
        # This handles cases where solvers return numpy arrays but is_solution expects lists
        if isinstance(result, dict):
            import numpy as np
            converted_result = {}
            for key, value in result.items():
                if isinstance(value, np.ndarray):
                    if value.size == 0:
                        converted_result[key] = []
                    else:
                        converted_result[key] = value.tolist()
                else:
                    converted_result[key] = value
            return converted_result
        
        return result

    return _wrapped_solver


def _prepare_dataset(
    task_instance: Task,
    split: str,
    max_samples: Optional[int],
) -> List[Any]:
    """Load dataset for the given task and split."""

    LOGGER.info("Loading dataset for task %s (split=%s)", task_instance.task_name, split)
    train_iter, test_iter = task_instance.load_dataset(train_size=10, test_size=10)
    dataset_iter: Iterable[Any] = train_iter if split == "train" else test_iter
    dataset_list = list(dataset_iter)

    if max_samples is not None:
        dataset_list = dataset_list[: max(0, max_samples)]

    if not dataset_list:
        raise RuntimeError(
            f"Dataset for task {task_instance.task_name} split '{split}' is empty."
        )

    LOGGER.info("Loaded %d problem instances", len(dataset_list))
    return dataset_list


def _load_generation_baseline(task_name: str, generation_file: Optional[Path]) -> Optional[float]:
    """Load average baseline timing from generation.json if available."""

    if not generation_file:
        return None

    try:
        with generation_file.open("r", encoding="utf-8") as handle:
            generation_data = json.load(handle)
    except FileNotFoundError:
        LOGGER.warning("Generation file %s not found", generation_file)
        return None
    except json.JSONDecodeError as exc:
        LOGGER.warning("Failed to parse %s: %s", generation_file, exc)
        return None

    task_entry = generation_data.get(task_name)
    if not isinstance(task_entry, dict):
        return None

    baseline_runs = task_entry.get("baseline_runs")
    if not isinstance(baseline_runs, dict) or not baseline_runs:
        return None

    averages: List[float] = []
    for run_info in baseline_runs.values():
        if not isinstance(run_info, dict):
            continue
        avg = run_info.get("avg_min_ms")
        if isinstance(avg, (int, float)):
            averages.append(float(avg))

    if not averages:
        return None

    return sum(averages) / float(len(averages))


def _extract_aggregate_metrics(results: Any) -> Dict[str, Any]:
    # Debug: Log type and content of results
    LOGGER.info(f"DEBUG_EXTRACT: results type = {type(results)}")
    LOGGER.info(f"DEBUG_EXTRACT: hasattr aggregate_metrics = {hasattr(results, 'aggregate_metrics')}")
    
    aggregate = getattr(results, "aggregate_metrics", {}) or {}
    
    # Debug: Log extracted aggregate
    LOGGER.info(f"DEBUG_EXTRACT: aggregate content = {aggregate}")

    metrics: Dict[str, Any] = {}
    metrics.update(aggregate)

    if metrics:
        num_evaluated = metrics.get("num_evaluated") or 0
        num_valid = metrics.get("num_valid") or 0
        accuracy = float(num_valid) / float(num_evaluated) if num_evaluated else 0.0
        metrics["accuracy"] = accuracy
    else:
        # Basic fallback when aggregate metrics are unavailable
        LOGGER.warning("DEBUG_EXTRACT: aggregate_metrics is empty, using fallback")
        if isinstance(results, Iterable):
            results_list = list(results)
            num_evaluated = len(results_list)
            num_valid = sum(1 for r in results_list if isinstance(r, dict) and r.get("is_valid"))
            accuracy = float(num_valid) / float(num_evaluated) if num_evaluated else 0.0
            metrics = {
                "num_evaluated": num_evaluated,
                "num_valid": num_valid,
                "accuracy": accuracy,
            }
        else:
            metrics = {
                "num_evaluated": 0,
                "num_valid": 0,
                "accuracy": 0.0,
            }

    return metrics


def _normalize_metric(value: Optional[float]) -> float:
    if value is None:
        return 0.0
    try:
        if isinstance(value, float):
            if value != value:  # NaN check
                return 0.0
            return value
        return float(value)
    except Exception:
        return 0.0


def _summarize_results(results: Any, aggregate_metrics: Dict[str, Any]) -> Dict[str, Any]:
    summary: Dict[str, Any] = {
        "aggregate_metrics": aggregate_metrics,
        "sample_results": [],
    }

    if isinstance(results, Iterable):
        sample = []
        for idx, item in enumerate(results):
            if idx >= 3:
                break
            sample.append(item)
        summary["sample_results"] = sample

    if hasattr(results, "invalid_solution_analysis"):
        summary["invalid_solution_analysis"] = getattr(
            results, "invalid_solution_analysis", []
        )

    return summary


def _load_train_baselines(generation_file: Optional[Path], task_name: str) -> List[float]:
    """
    Load per-problem baseline times from train_baseline.json.
    
    Args:
        generation_file: Path to generation.json (used to locate train_baseline.json)
        task_name: Name of the task
        
    Returns:
        List of baseline times (ms) for each problem
    """
    if generation_file:
        baseline_file = generation_file.parent / 'train_baseline.json'
    else:
        baseline_file = Path("/data/zq/evolve/reports/train_baseline.json")
    
    if not baseline_file.exists():
        raise FileNotFoundError(
            f"train_baseline.json not found at {baseline_file}\n"
            f"Please generate train baseline first."
        )
    
    LOGGER.info(f"Loading per-problem TRAIN baselines from {baseline_file}")
    
    with open(baseline_file, 'r') as f:
        data = json.load(f)
    
    if task_name not in data:
        raise ValueError(f"Task '{task_name}' not found in {baseline_file}")
    
    task_data = data[task_name]
    
    if 'problem_min_times' not in task_data:
        raise ValueError(f"'problem_min_times' not found in train_baseline.json for task '{task_name}'")
    
    per_problem_baselines = task_data['problem_min_times']
    
    LOGGER.info(f"Loaded {len(per_problem_baselines)} per-problem baselines")
    
    return per_problem_baselines


def evaluate(program_path: str, config: Optional[Dict[str, Any]] = None) -> EvaluationResult:
    """
    Evaluate a solver implementation against the AlgoTune TRAIN dataset.
    
    Evaluation methodology (matches save_eval_to_summary.py):
    - Each problem: warmup once (using different problem), run 10 times in isolated subprocesses
    - Each subprocess: warmup + timed call, then exit
    - Result: min time across all 10 runs for each problem
    - Speedup calculation:
      1. For each problem i: problem_speedup_i = train_baseline_min_time_i / solver_min_time_i
      2. Task speedup = mean([problem_speedup_1, problem_speedup_2, ..., problem_speedup_N])
    - This speedup is used as combined_score for OpenEvolve ranking
    """

    resolved_config = _resolve_config(config)
    task_name = resolved_config["task_name"]
    data_dir: Path = resolved_config["data_dir"]
    split: str = resolved_config.get("split", "train")  # Force train for OpenEvolve
    max_samples: Optional[int] = resolved_config.get("max_samples")
    generation_file: Optional[Path] = resolved_config.get("generation_file")
    num_runs: int = resolved_config.get("num_runs", 10)

    # Set up environment for isolated execution (matches save_eval_to_summary.py)
    os.environ.setdefault("DATA_DIR", str(data_dir))
    os.environ.setdefault("ALGO_TUNE_DATA_DIR", str(data_dir))
    os.environ.setdefault("CURRENT_TASK_NAME", task_name)
    os.environ.setdefault("ISOLATED_EVAL", "1")  # Use isolated execution
    os.environ.setdefault("SKIP_DATASET_GEN", "1")

    LOGGER.info("Evaluating solver at %s for task %s (TRAIN dataset)", program_path, task_name)

    # Load per-problem baselines from train_baseline.json
    per_problem_baselines = _load_train_baselines(generation_file, task_name)

    # Import required modules
    from AlgoTuneTasks.factory import TaskFactory
    from AlgoTuner.utils.discover_and_list_tasks import discover_and_import_tasks
    from AlgoTuner.utils.serialization import dataset_decoder
    from AlgoTuner.utils.isolated_benchmark import run_isolated_benchmark
    import glob
    import shutil
    import statistics

    discover_and_import_tasks()
    
    task_instance = TaskFactory(task_name, data_dir=str(data_dir))
    task_instance.task_name = task_name
    
    LOGGER.info(f"Loading TRAIN dataset for {task_name}")
    LOGGER.info(f"Using isolated execution (subprocess) to match baseline methodology")
    
    # Load TRAIN dataset
    data_files = glob.glob(str(data_dir / "**" / f"{task_name}*_train.jsonl"), recursive=True)
    if not data_files:
        data_files = glob.glob(str(data_dir / f"{task_name}" / f"*_train.jsonl"))
    
    if not data_files:
        raise FileNotFoundError(f"No train JSONL file found for {task_name} in {data_dir}")
    
    train_file = data_files[0]
    LOGGER.info(f"Loading TRAIN data from: {train_file}")
    
    train_base_dir = os.path.dirname(train_file)
    
    # Load dataset with full structure
    train_dataset_items = []
    with open(train_file, 'r') as f:
        for line in f:
            if line.strip():
                raw_data = json.loads(line)
                decoded_data = dataset_decoder(raw_data, base_dir=train_base_dir)
                train_dataset_items.append(decoded_data)
    
    LOGGER.info(f"Loaded {len(train_dataset_items)} train problems from file")
    
    if len(train_dataset_items) != len(per_problem_baselines):
        raise ValueError(
            f"Mismatch: {len(per_problem_baselines)} baselines but {len(train_dataset_items)} train problems"
        )
    
    # Get solver code directory
    solver_path_obj = Path(program_path)
    code_dir = str(solver_path_obj.parent)
    solver_filename = solver_path_obj.name
    
    if not solver_path_obj.exists():
        raise FileNotFoundError(f"Solver file not found: {program_path}")
    
    # Ensure solver.py exists in code_dir (for isolated benchmark)
    code_dir_path = Path(code_dir)
    standard_names = [f"{task_name}.py", "solver.py"]
    if solver_filename not in standard_names:
        standard_solver_path = code_dir_path / "solver.py"
        if not standard_solver_path.exists():
            shutil.copy2(solver_path_obj, standard_solver_path)
            LOGGER.info(f"Created solver.py copy from {solver_filename} for isolated benchmark")
    
    LOGGER.info(f"Solver code directory: {code_dir}")
    
    # Evaluate each problem
    solver_results = []
    problem_count = len(train_dataset_items)
    
    for idx, item in enumerate(train_dataset_items):
        # Extract problem data and ID
        if isinstance(item, dict):
            problem_id = item.get("id", item.get("seed", item.get("k", None)))
            if problem_id is None:
                problem_id = f"problem_{idx+1}"
            problem_id = str(problem_id)
            problem_data = item.get('problem', item)
        else:
            problem_id = f"problem_{idx+1}"
            problem_data = item
        
        LOGGER.debug(f"Evaluating problem {idx+1}/{problem_count} (ID: {problem_id})")
        
        # Get warmup problem (use next problem in dataset, wrapping around)
        warmup_idx = (idx + 1) % problem_count
        warmup_item = train_dataset_items[warmup_idx]
        warmup_problem_data = warmup_item.get('problem', warmup_item) if isinstance(warmup_item, dict) else warmup_item
        
        # Calculate timeout
        timeout_seconds = 60.0
        if hasattr(task_instance, 'target_time_ms') and task_instance.target_time_ms:
            target_time_s = task_instance.target_time_ms / 1000.0
            timeout_seconds = max(60.0, target_time_s * 10.0)
        
        try:
            # Use isolated benchmark (matches baseline generation methodology)
            benchmark_result = run_isolated_benchmark(
                task_name=task_name,
                code_dir=code_dir,
                warmup_problem=warmup_problem_data,
                timed_problem=problem_data,
                num_runs=num_runs,
                timeout_seconds=timeout_seconds,
            )
            
            if benchmark_result.get('success'):
                min_time_ms = benchmark_result.get('min_time_ms', 0)
                
                if min_time_ms > 0:
                    # Validate solution
                    result = benchmark_result.get('result')
                    is_valid = task_instance.is_solution(problem_data, result) if result is not None else False
                    
                    solver_results.append({
                        'problem_id': problem_id,
                        'min_time_ms': min_time_ms,
                        'is_valid': is_valid,
                        'status': 'success',
                        'error': None
                    })
                else:
                    solver_results.append({
                        'problem_id': problem_id,
                        'min_time_ms': None,
                        'is_valid': False,
                        'status': 'failed',
                        'error': 'No valid timing result'
                    })
            else:
                error_msg = benchmark_result.get('error', 'Unknown error')
                LOGGER.warning(f"Problem {problem_id} FAILED: {error_msg}")
                solver_results.append({
                    'problem_id': problem_id,
                    'min_time_ms': None,
                    'is_valid': False,
                    'status': 'failed',
                    'error': error_msg
                })
        
        except Exception as e:
            error_msg = f"{type(e).__name__}: {str(e)}"
            LOGGER.warning(f"Problem {problem_id} FAILED: {error_msg}")
            solver_results.append({
                'problem_id': problem_id,
                'min_time_ms': None,
                'is_valid': False,
                'status': 'failed',
                'error': error_msg
            })
    
    # Calculate metrics using AlgoTune official methodology
    num_problems = len(solver_results)
    num_valid = sum(1 for r in solver_results if r['is_valid'])
    num_failed = sum(1 for r in solver_results if r['status'] == 'failed')
    
    # Calculate per-problem speedups
    per_problem_speedups = []
    problem_details = []
    
    for i in range(num_problems):
        baseline_i = per_problem_baselines[i]
        result = solver_results[i]
        
        if result['status'] == 'success' and result['min_time_ms'] is not None and result['min_time_ms'] > 0:
            speedup_i = baseline_i / result['min_time_ms']
            per_problem_speedups.append(speedup_i)
            problem_details.append({
                'problem_id': result['problem_id'],
                'baseline_time_ms': baseline_i,
                'solver_time_ms': result['min_time_ms'],
                'speedup': speedup_i,
                'is_valid': result['is_valid']
            })
        else:
            # Failed problems: speedup = 0.0
            problem_details.append({
                'problem_id': result['problem_id'],
                'baseline_time_ms': baseline_i,
                'solver_time_ms': result['min_time_ms'],
                'speedup': 0.0,
                'is_valid': False,
                'error': result['error']
            })
    
    # Calculate final speedup: mean of per-problem speedups
    if per_problem_speedups:
        final_speedup = statistics.mean(per_problem_speedups)
    else:
        final_speedup = 0.0
    
    # If accuracy is 0, speedup must be 0
    accuracy = num_valid / num_problems if num_problems > 0 else 0.0
    if accuracy == 0.0:
        final_speedup = 0.0
    
    # Calculate other metrics
    baseline_avg_min_ms = statistics.mean(per_problem_baselines)
    successful_solver_times = [r['min_time_ms'] for r in solver_results if r['status'] == 'success' and r['min_time_ms'] is not None]
    if successful_solver_times:
        solver_avg_min_ms = statistics.mean(successful_solver_times)
    else:
        solver_avg_min_ms = 0.0
    
    LOGGER.info("=" * 70)
    LOGGER.info("METRICS (AlgoTune Official Methodology - TRAIN dataset):")
    LOGGER.info(f"  Final Speedup:       {final_speedup:.4f}x ⭐")
    LOGGER.info(f"    = mean([problem_speedup_i]) where problem_speedup_i = baseline_min_i / solver_min_i")
    LOGGER.info(f"    Each problem: warmup once, run {num_runs} times, take min time")
    LOGGER.info(f"  Baseline avg:        {baseline_avg_min_ms:.4f}ms (reference)")
    LOGGER.info(f"  Solver avg:          {solver_avg_min_ms:.4f}ms")
    LOGGER.info(f"  Problems:            {num_problems} total")
    LOGGER.info(f"    ✓ Valid:           {num_valid}")
    LOGGER.info(f"    ✗ Failed:          {num_failed}")
    LOGGER.info("=" * 70)
    
    # Return metrics compatible with OpenEvolve
    metrics = {
        "combined_score": final_speedup,  # Used by OpenEvolve for ranking (mean of per-problem speedups)
        "mean_speedup": final_speedup,
        "num_valid": float(num_valid),
        "success_rate": float(num_valid) / float(num_problems) if num_problems > 0 else 0.0,
        "accuracy": accuracy,
    }
    
    summary = {
        "speedup": final_speedup,
        "baseline_avg_min_ms": baseline_avg_min_ms,
        "solver_avg_min_ms": solver_avg_min_ms,
        "num_problems": num_problems,
        "num_valid": num_valid,
        "num_failed": num_failed,
        "accuracy": accuracy,
        "problem_results": problem_details,
        "config": resolved_config,
    }
    
    artifacts = {
        "summary.json": _safe_json(summary),
    }
    
    return EvaluationResult(metrics=metrics, artifacts=artifacts)


def _cli() -> None:
    parser = argparse.ArgumentParser(description="Evaluate an AlgoTune solver.py file")
    parser.add_argument("program", help="Path to the solver implementation (solver.py)")
    parser.add_argument("--task", dest="task", help="AlgoTune task name (overrides env)")
    parser.add_argument(
        "--data-dir",
        dest="data_dir",
        type=Path,
        help="Directory containing generated AlgoTune datasets",
    )
    parser.add_argument(
        "--split",
        choices=["train", "test"],
        help="Dataset split to evaluate (default: env or test)",
    )
    parser.add_argument(
        "--max-samples",
        type=int,
        help="Limit number of dataset samples (useful for quick checks)",
    )
    parser.add_argument(
        "--test-mode",
        action="store_true",
        help="Enable AlgoTune test mode (limits evaluation to 10 samples)",
    )
    parser.add_argument("--num-runs", type=int, help="Override benchmark runs")
    parser.add_argument("--warmup-runs", type=int, help="Override warmup runs")
    parser.add_argument(
        "--generation-file",
        type=Path,
        help="Path to generation.json for baseline context",
    )

    args = parser.parse_args()

    overrides: Dict[str, Any] = {}
    if args.task:
        overrides["task_name"] = args.task
    if args.data_dir:
        overrides["data_dir"] = args.data_dir
    if args.split:
        overrides["split"] = args.split
    if args.max_samples is not None:
        overrides["max_samples"] = args.max_samples
    if args.test_mode:
        overrides["test_mode"] = True
    if args.num_runs is not None:
        overrides["num_runs"] = args.num_runs
    if args.warmup_runs is not None:
        overrides["warmup_runs"] = args.warmup_runs
    if args.generation_file is not None:
        overrides["generation_file"] = args.generation_file

    result = evaluate(args.program, config=overrides)
    print(json.dumps(result.metrics, indent=2))
    if result.artifacts:
        print("\nArtifacts:")
        for key in result.artifacts:
            print(f"- {key}")


if __name__ == "__main__":
    _cli()
