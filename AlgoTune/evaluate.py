"""Unified evaluation entry point for AlgoTune solvers.

This module exposes an ``evaluate`` function compatible with OpenEvolve's
``Evaluator`` while remaining convenient for direct command line usage.
It loads datasets generated by AlgoTune tasks, runs a candidate ``solver.py``
implementation against them, and reports aggregate metrics such as mean
speedup and accuracy using AlgoTune's native evaluation pipeline.

Under the hood the adapter resolves task configuration, instantiates the
original AlgoTune :class:`Task` objects, and delegates execution to
``AlgoTuner.utils.evaluator.main.evaluate_code_on_dataset``â€”the same function
used by AlgoTune's internal evaluator.  The candidate solver is dynamically
imported and bound onto a task instance so the native evaluator can execute it
without modifications.

The evaluator expects a task name via configuration (either the ``config``
argument or environment variable ``ALGO_TUNE_TASK``) and a data directory
containing the generated ``*.jsonl`` files (defaults to
``/data/zq/evolve/AlgoTune/data``).  Results are returned as an
``EvaluationResult`` so that OpenEvolve can store both metrics and optional
artifacts.
"""

from __future__ import annotations

import argparse
import importlib
import importlib.util
import json
import logging
import os
import sys
from pathlib import Path
from typing import Any, Callable, Dict, Iterable, List, Optional

from openevolve.evaluation_result import EvaluationResult

# Ensure the AlgoTune repository (this file's parent) is importable
REPO_ROOT = Path(__file__).resolve().parent
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from AlgoTuneTasks.base import TASK_REGISTRY, Task  # type: ignore  # noqa: E402
from AlgoTuner.utils.evaluator.baseline_manager import BaselineManager  # type: ignore  # noqa: E402
from AlgoTuner.utils.evaluator.main import evaluate_code_on_dataset  # type: ignore  # noqa: E402

# Configure a module-level logger
LOGGER = logging.getLogger("algotune.evaluate")
if not LOGGER.handlers:
    logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")


DEFAULT_DATA_DIR = Path("/data/zq/evolve/AlgoTune/data")
DEFAULT_GENERATION_FILE = Path("/data/zq/evolve/reports/generation.json")


def _parse_int(value: Optional[str]) -> Optional[int]:
    if value is None or value == "":
        return None
    try:
        return int(value)
    except ValueError:
        LOGGER.warning("Unable to parse integer from %s", value)
        return None


def _parse_float(value: Optional[str]) -> Optional[float]:
    if value is None or value == "":
        return None
    try:
        return float(value)
    except ValueError:
        LOGGER.warning("Unable to parse float from %s", value)
        return None


def _resolve_config(overrides: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """Resolve configuration from overrides and environment variables."""

    config: Dict[str, Any] = {
        "task_name": os.environ.get("ALGO_TUNE_TASK"),
        "data_dir": os.environ.get("ALGO_TUNE_DATA_DIR"),
        "split": os.environ.get("ALGO_TUNE_SPLIT", "test"),
        "max_samples": _parse_int(os.environ.get("ALGO_TUNE_MAX_SAMPLES")),
        "timeout_multiplier": _parse_float(os.environ.get("ALGO_TUNE_TIMEOUT_MULTIPLIER")),
        "min_timeout_seconds": _parse_float(os.environ.get("ALGO_TUNE_MIN_TIMEOUT")),
        "max_timeout_seconds": _parse_float(os.environ.get("ALGO_TUNE_MAX_TIMEOUT")),
        "num_runs": _parse_int(os.environ.get("ALGO_TUNE_NUM_RUNS")),
        "warmup_runs": _parse_int(os.environ.get("ALGO_TUNE_WARMUP_RUNS")),
        "test_mode": os.environ.get("ALGO_TUNE_TEST_MODE", "0") == "1",
        "generation_file": os.environ.get("ALGO_TUNE_GENERATION_FILE"),
    }

    if overrides:
        for key, value in overrides.items():
            if value is not None:
                config[key] = value

    # Task name is mandatory
    if not config.get("task_name"):
        raise ValueError(
            "Task name not provided. Set ALGO_TUNE_TASK env var or pass task_name in config."
        )

    data_dir = config.get("data_dir")
    if data_dir:
        config["data_dir"] = Path(data_dir)
    else:
        config["data_dir"] = DEFAULT_DATA_DIR

    generation_file = config.get("generation_file")
    if generation_file:
        generation_path = Path(generation_file)
        config["generation_file"] = generation_path
    elif DEFAULT_GENERATION_FILE.exists():
        config["generation_file"] = DEFAULT_GENERATION_FILE
    else:
        config["generation_file"] = None

    if config["split"] not in {"train", "test"}:
        raise ValueError("split must be 'train' or 'test'")

    return config


def _import_task_module(task_name: str) -> None:
    """Import the AlgoTune task module to populate TASK_REGISTRY."""

    if task_name in TASK_REGISTRY:
        return

    module_name = f"AlgoTuneTasks.{task_name}.{task_name}"
    LOGGER.info("Importing task module %s", module_name)
    importlib.import_module(module_name)


def _load_task_class(task_name: str) -> type[Task]:
    _import_task_module(task_name)
    if task_name not in TASK_REGISTRY:
        available = ", ".join(sorted(TASK_REGISTRY.keys()))
        raise ValueError(f"Task '{task_name}' not found in TASK_REGISTRY. Available: {available}")
    task_cls = TASK_REGISTRY[task_name]
    if not issubclass(task_cls, Task):
        raise TypeError(f"Registered task '{task_name}' is not a Task subclass")
    return task_cls


def _safe_json(data: Any) -> str:
    """Serialize arbitrary data to JSON, converting non-serializable values."""

    def convert(obj: Any) -> Any:
        if isinstance(obj, (str, int, float, bool)) or obj is None:
            return obj
        if isinstance(obj, Path):
            return str(obj)
        if isinstance(obj, dict):
            return {str(k): convert(v) for k, v in obj.items()}
        if isinstance(obj, (list, tuple, set)):
            return [convert(v) for v in obj]
        if hasattr(obj, "_asdict"):
            return convert(obj._asdict())
        if hasattr(obj, "__dict__"):
            try:
                return convert(vars(obj))
            except Exception:  # pragma: no cover - fallback
                pass
        return repr(obj)

    return json.dumps(convert(data), indent=2, ensure_ascii=False)


def _load_solver_callable(
    program_path: str,
    task_class: type[Task],
    candidate_task: Task,
) -> Callable[[Any], Any]:
    """Load a solver callable from ``program_path``.

    Supports multiple patterns:
      * A top-level ``solve(problem)`` function
      * A ``run_solver(problem)`` function
      * A ``Solver`` class with a ``solve`` method
      * A class matching the task name that implements ``solve``
    """

    spec = importlib.util.spec_from_file_location("candidate_solver", program_path)
    if spec is None or spec.loader is None:
        raise ImportError(f"Could not load solver module from {program_path}")

    module = importlib.util.module_from_spec(spec)
    sys.modules[spec.name] = module
    spec.loader.exec_module(module)

    candidates: List[Callable[..., Any]] = []

    for attr_name in ("solve", "run_solver"):
        candidate = getattr(module, attr_name, None)
        if callable(candidate):
            candidates.append(candidate)

    solver_cls = getattr(module, "Solver", None)
    if solver_cls is not None:
        try:
            instance = solver_cls()
            if hasattr(instance, "solve") and callable(instance.solve):
                candidates.append(instance.solve)
        except Exception as exc:  # pragma: no cover - defensive
            LOGGER.warning("Failed to instantiate Solver class: %s", exc)

    task_cls_name = task_class.__name__
    module_task_cls = getattr(module, task_cls_name, None)
    if module_task_cls is not None:
        try:
            instance = module_task_cls()
            if hasattr(instance, "solve") and callable(instance.solve):
                candidates.append(instance.solve)
        except Exception as exc:  # pragma: no cover - defensive
            LOGGER.warning("Failed to instantiate %s from solver module: %s", task_cls_name, exc)

    if not candidates:
        raise AttributeError(
            "Solver module must define one of: solve(), run_solver(), Solver.solve(), "
            f"or {task_cls_name}.solve()"
        )

    solver_callable = candidates[0]

    def _wrapped_solver(problem: Any) -> Any:
        try:
            return solver_callable(problem)
        except TypeError as exc:
            # Some solvers expect the task instance as an additional argument
            if "missing" in str(exc) and "positional argument" in str(exc):
                return solver_callable(problem, candidate_task)
            raise

    return _wrapped_solver


def _prepare_dataset(
    task_instance: Task,
    split: str,
    max_samples: Optional[int],
) -> List[Any]:
    """Load dataset for the given task and split."""

    LOGGER.info("Loading dataset for task %s (split=%s)", task_instance.task_name, split)
    train_iter, test_iter = task_instance.load_dataset()
    dataset_iter: Iterable[Any] = train_iter if split == "train" else test_iter
    dataset_list = list(dataset_iter)

    if max_samples is not None:
        dataset_list = dataset_list[: max(0, max_samples)]

    if not dataset_list:
        raise RuntimeError(
            f"Dataset for task {task_instance.task_name} split '{split}' is empty."
        )

    LOGGER.info("Loaded %d problem instances", len(dataset_list))
    return dataset_list


def _load_generation_baseline(task_name: str, generation_file: Optional[Path]) -> Optional[float]:
    """Load average baseline timing from generation.json if available."""

    if not generation_file:
        return None

    try:
        with generation_file.open("r", encoding="utf-8") as handle:
            generation_data = json.load(handle)
    except FileNotFoundError:
        LOGGER.warning("Generation file %s not found", generation_file)
        return None
    except json.JSONDecodeError as exc:
        LOGGER.warning("Failed to parse %s: %s", generation_file, exc)
        return None

    task_entry = generation_data.get(task_name)
    if not isinstance(task_entry, dict):
        return None

    baseline_runs = task_entry.get("baseline_runs")
    if not isinstance(baseline_runs, dict) or not baseline_runs:
        return None

    averages: List[float] = []
    for run_info in baseline_runs.values():
        if not isinstance(run_info, dict):
            continue
        avg = run_info.get("avg_min_ms")
        if isinstance(avg, (int, float)):
            averages.append(float(avg))

    if not averages:
        return None

    return sum(averages) / float(len(averages))


def _extract_aggregate_metrics(results: Any) -> Dict[str, Any]:
    aggregate = getattr(results, "aggregate_metrics", {}) or {}

    metrics: Dict[str, Any] = {}
    metrics.update(aggregate)

    if metrics:
        num_evaluated = metrics.get("num_evaluated") or 0
        num_valid = metrics.get("num_valid") or 0
        accuracy = float(num_valid) / float(num_evaluated) if num_evaluated else 0.0
        metrics["accuracy"] = accuracy
    else:
        # Basic fallback when aggregate metrics are unavailable
        if isinstance(results, Iterable):
            results_list = list(results)
            num_evaluated = len(results_list)
            num_valid = sum(1 for r in results_list if isinstance(r, dict) and r.get("is_valid"))
            accuracy = float(num_valid) / float(num_evaluated) if num_evaluated else 0.0
            metrics = {
                "num_evaluated": num_evaluated,
                "num_valid": num_valid,
                "accuracy": accuracy,
            }
        else:
            metrics = {
                "num_evaluated": 0,
                "num_valid": 0,
                "accuracy": 0.0,
            }

    return metrics


def _normalize_metric(value: Optional[float]) -> float:
    if value is None:
        return 0.0
    try:
        if isinstance(value, float):
            if value != value:  # NaN check
                return 0.0
            return value
        return float(value)
    except Exception:
        return 0.0


def _summarize_results(results: Any, aggregate_metrics: Dict[str, Any]) -> Dict[str, Any]:
    summary: Dict[str, Any] = {
        "aggregate_metrics": aggregate_metrics,
        "sample_results": [],
    }

    if isinstance(results, Iterable):
        sample = []
        for idx, item in enumerate(results):
            if idx >= 3:
                break
            sample.append(item)
        summary["sample_results"] = sample

    if hasattr(results, "invalid_solution_analysis"):
        summary["invalid_solution_analysis"] = getattr(
            results, "invalid_solution_analysis", []
        )

    return summary


def evaluate(program_path: str, config: Optional[Dict[str, Any]] = None) -> EvaluationResult:
    """Evaluate a solver implementation against the AlgoTune dataset."""

    resolved_config = _resolve_config(config)
    task_name = resolved_config["task_name"]
    data_dir: Path = resolved_config["data_dir"]
    split: str = resolved_config["split"]
    max_samples: Optional[int] = resolved_config.get("max_samples")
    generation_file: Optional[Path] = resolved_config.get("generation_file")

    # Set up environment for AlgoTune helpers
    os.environ.setdefault("DATA_DIR", str(data_dir))
    os.environ.setdefault("ALGO_TUNE_DATA_DIR", str(data_dir))
    os.environ.setdefault("CURRENT_TASK_NAME", task_name)
    os.environ.setdefault("AGENT_MODE", "0")  # Ensure in-process execution
    os.environ.setdefault("ISOLATED_EVAL", "0")
    os.environ.setdefault("SKIP_DATASET_GEN", "1")

    LOGGER.info("Evaluating solver at %s for task %s", program_path, task_name)

    task_class = _load_task_class(task_name)

    # Prepare baseline task (unmodified) for baseline timings
    baseline_task = task_class(data_dir=str(data_dir))
    dataset_list = _prepare_dataset(baseline_task, split=split, max_samples=max_samples)

    baseline_manager = BaselineManager(baseline_task)

    # Prepare candidate task with patched solve method
    candidate_task = task_class(data_dir=str(data_dir))
    solver_callable = _load_solver_callable(program_path, task_class, candidate_task)
    candidate_task.solve = solver_callable  # type: ignore[attr-defined]
    candidate_task.oracle = solver_callable  # type: ignore[attr-defined]

    eval_kwargs = {
        "task_obj": candidate_task,
        "dataset_iterable": dataset_list,
        "baseline_manager": baseline_manager,
        "data_subset": split,
        "test_mode": resolved_config.get("test_mode", False),
    }

    # Optional overrides propagated to evaluate_code_on_dataset
    if resolved_config.get("timeout_multiplier") is not None:
        eval_kwargs["timeout_multiplier"] = resolved_config["timeout_multiplier"]
    if resolved_config.get("min_timeout_seconds") is not None:
        eval_kwargs["min_timeout_seconds"] = resolved_config["min_timeout_seconds"]
    if resolved_config.get("max_timeout_seconds") is not None:
        eval_kwargs["max_timeout_seconds"] = resolved_config["max_timeout_seconds"]
    if resolved_config.get("num_runs") is not None:
        eval_kwargs["default_num_eval_runs"] = resolved_config["num_runs"]
    if resolved_config.get("warmup_runs") is not None:
        eval_kwargs["default_num_warmup_runs"] = resolved_config["warmup_runs"]

    try:
        results = evaluate_code_on_dataset(**eval_kwargs)
    except Exception as exc:
        LOGGER.error("Evaluation failed: %s", exc)
        metrics = {
            "score": 0.0,
            "mean_speedup": 0.0,
            "median_speedup": 0.0,
            "accuracy": 0.0,
            "num_evaluated": 0.0,
            "num_valid": 0.0,
            "num_errors": 1.0,
        }
        artifacts = {
            "error": str(exc),
            "config": _safe_json(resolved_config),
        }
        return EvaluationResult(metrics=metrics, artifacts=artifacts)

    if isinstance(results, dict):
        LOGGER.warning("Evaluation returned error structure: %s", results.get("error"))
        metrics = {
            "score": 0.0,
            "mean_speedup": 0.0,
            "median_speedup": 0.0,
            "accuracy": 0.0,
            "num_evaluated": float(results.get("problems_evaluated", 0) or 0),
            "num_valid": 0.0,
            "num_errors": 1.0,
        }
        artifacts = {
            "error": _safe_json(results),
            "config": _safe_json(resolved_config),
        }
        return EvaluationResult(metrics=metrics, artifacts=artifacts)

    aggregate_metrics = _extract_aggregate_metrics(results)

    mean_speedup = _normalize_metric(aggregate_metrics.get("mean_speedup"))
    median_speedup = _normalize_metric(aggregate_metrics.get("median_speedup"))
    accuracy = _normalize_metric(aggregate_metrics.get("accuracy"))
    success_rate = _normalize_metric(aggregate_metrics.get("success_rate"))
    num_evaluated = _normalize_metric(aggregate_metrics.get("num_evaluated"))
    num_valid = _normalize_metric(aggregate_metrics.get("num_valid"))
    num_errors = _normalize_metric(aggregate_metrics.get("num_errors"))
    num_timeouts = _normalize_metric(aggregate_metrics.get("num_timeouts"))
    avg_solver_time_ms = _normalize_metric(aggregate_metrics.get("avg_solver_time_ms"))
    avg_baseline_time_ms = _normalize_metric(aggregate_metrics.get("avg_oracle_time_ms"))

    solver_to_baseline_ratio = (
        avg_solver_time_ms / avg_baseline_time_ms
        if avg_baseline_time_ms > 0
        else 0.0
    )

    score = mean_speedup if mean_speedup > 0 else accuracy

    metrics = {
        "score": score,
        "mean_speedup": mean_speedup,
        "median_speedup": median_speedup,
        "accuracy": accuracy,
        "success_rate": success_rate,
        "num_evaluated": num_evaluated,
        "num_valid": num_valid,
        "num_errors": num_errors,
        "num_timeouts": num_timeouts,
        "avg_solver_time_ms": avg_solver_time_ms,
        "avg_baseline_time_ms": avg_baseline_time_ms,
        "solver_to_baseline_ratio": solver_to_baseline_ratio,
    }

    summary = _summarize_results(results, aggregate_metrics)
    summary["config"] = resolved_config
    summary["generation_avg_baseline_ms"] = _load_generation_baseline(task_name, generation_file)

    artifacts = {
        "summary.json": _safe_json(summary),
    }

    return EvaluationResult(metrics=metrics, artifacts=artifacts)


def _cli() -> None:
    parser = argparse.ArgumentParser(description="Evaluate an AlgoTune solver.py file")
    parser.add_argument("program", help="Path to the solver implementation (solver.py)")
    parser.add_argument("--task", dest="task", help="AlgoTune task name (overrides env)")
    parser.add_argument(
        "--data-dir",
        dest="data_dir",
        type=Path,
        help="Directory containing generated AlgoTune datasets",
    )
    parser.add_argument(
        "--split",
        choices=["train", "test"],
        help="Dataset split to evaluate (default: env or test)",
    )
    parser.add_argument(
        "--max-samples",
        type=int,
        help="Limit number of dataset samples (useful for quick checks)",
    )
    parser.add_argument(
        "--test-mode",
        action="store_true",
        help="Enable AlgoTune test mode (limits evaluation to 10 samples)",
    )
    parser.add_argument("--num-runs", type=int, help="Override benchmark runs")
    parser.add_argument("--warmup-runs", type=int, help="Override warmup runs")
    parser.add_argument(
        "--generation-file",
        type=Path,
        help="Path to generation.json for baseline context",
    )

    args = parser.parse_args()

    overrides: Dict[str, Any] = {}
    if args.task:
        overrides["task_name"] = args.task
    if args.data_dir:
        overrides["data_dir"] = args.data_dir
    if args.split:
        overrides["split"] = args.split
    if args.max_samples is not None:
        overrides["max_samples"] = args.max_samples
    if args.test_mode:
        overrides["test_mode"] = True
    if args.num_runs is not None:
        overrides["num_runs"] = args.num_runs
    if args.warmup_runs is not None:
        overrides["warmup_runs"] = args.warmup_runs
    if args.generation_file is not None:
        overrides["generation_file"] = args.generation_file

    result = evaluate(args.program, config=overrides)
    print(json.dumps(result.metrics, indent=2))
    if result.artifacts:
        print("\nArtifacts:")
        for key in result.artifacts:
            print(f"- {key}")


if __name__ == "__main__":
    _cli()
