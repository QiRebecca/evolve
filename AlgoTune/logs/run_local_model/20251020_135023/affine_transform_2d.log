Attempting to load config from: /data/zq/evolve/AlgoTune/AlgoTuner/config/config.yaml
Successfully loaded config from: /data/zq/evolve/AlgoTune/AlgoTuner/config/config.yaml

[1;31mProvider List: https://docs.litellm.ai/docs/providers[0m

Traceback (most recent call last):
  File "/data/zq/evolve/AlgoTune/scripts/run_local_model.py", line 473, in <module>
    main()
  File "/data/zq/evolve/AlgoTune/scripts/run_local_model.py", line 459, in main
    final_path = run_local_model(
  File "/data/zq/evolve/AlgoTune/scripts/run_local_model.py", line 397, in run_local_model
    response = litellm.completion(**completion_params)
  File "/home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/litellm/utils.py", line 1367, in wrapper
    raise e
  File "/home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/litellm/utils.py", line 1240, in wrapper
    result = original_function(*args, **kwargs)
  File "/home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/litellm/main.py", line 3760, in completion
    raise exception_type(
  File "/home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/litellm/main.py", line 1152, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
  File "/home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 418, in get_llm_provider
    raise e
  File "/home/zhangqi/.conda/envs/env/lib/python3.10/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py", line 395, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=chatgptoss-20b
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
