llm:
  # Configure the primary evolution model (defaults to OPENAI_API_KEY for auth)
  api_base: "https://api.shubiaobiao.cn/v1"  # 使用代理 API base（因为无法直接访问 api.openai.com）
  # api_base: "https://api.openai.com/v1"  # 直接访问（如果网络允许）
  reasoning_effort: "medium"
  max_tokens: 100000  # Maximum supported by API (was 128000 but API limit is 100000)
  # Request parameters - o3 is a reasoning model that needs longer timeout
  timeout: 600  # 10 minutes timeout for o3 reasoning model
  retries: 3
  retry_delay: 5
  models:
    - name: "o3"  # 暂时改用gpt-4o，等o3 API key修复后再改回
      weight: 1.0
      timeout: 600  # Extended timeout for reasoning model

  # Reuse the same model for evaluator feedback unless overridden
  evaluator_models:
    - name: "o3"  # 暂时改用gpt-4o
      weight: 1.0
      timeout: 600  # Extended timeout for reasoning model

# Evolution settings
diff_based_evolution: false  # Use full code rewrites instead of diffs (better for o3 model)

# Database settings
database:
  log_prompts: true  # Save prompts and LLM responses for each program

# Evolution trace settings
evolution_trace:
  enabled: true                 # Enable evolution trace logging
  format: 'jsonl'              # Output format (jsonl for streaming)
  include_code: true           # Include full program code in traces
  include_prompts: true        # Include prompts and LLM responses (including reasoning)
  output_path: null            # Will default to output_dir/evolution_trace.jsonl
  buffer_size: 1               # Write immediately (no buffering)
  compress: false              # Don't compress for easier reading

prompt:
  task_name: null  # Defaults to $ALGO_TUNE_TASK if unset
  task_library_root: null  # Defaults to repo/env fallbacks (e.g. AlgoTune/AlgoTuneTasks)
  evaluator_system_message: |-
    You are the performance judge for OpenEvolve runs that target AlgoTune tasks.
    
    Focus on these key metrics:
    
    • Speedup (mean_speedup) – Your PRIMARY metric. Higher is better. 
      - Speedup = Baseline Time / Solver Time
      - 1.0x = same speed as baseline
      - 1.5x = 50% faster than baseline (GOOD!)
      - 2.0x = 2x faster than baseline (EXCELLENT!)
      - < 1.0x = slower than baseline (BAD!)
    
    • Accuracy (num_valid/num_evaluated) – Must be 100%. Any errors are unacceptable.
    
    When comparing candidates:
    1. REQUIRE 100% accuracy (num_errors must be 0, num_timeouts must be 0)
    2. SELECT the one with HIGHER speedup
    3. Reject any candidate with errors or timeouts, even if faster
    
    Keep your feedback concise and focus on speedup improvements.
  system_message: |-
    You're an evolutionary coding agent optimizing algorithm performance. Your goal is to maximize SPEEDUP while maintaining 100% correctness.
    
    KEY METRIC: Speedup = Baseline Time / Your Time
    - 1.0x = same speed (no improvement)
    - 1.5x = 50% faster (good!)
    - 2.0x = 2x faster (excellent!)
    - < 1.0x = slower (bad!)
    
    CRITICAL RULES:
    - Maintain 100% correctness (num_errors must be 0)
    - Focus on SPEEDUP as your primary objective
    - Faster is better, but only if all tests pass
    
    Every message you send incurs a cost--you will be informed of your usage and remaining budget.
    Apart from the default Python packages, you have access to the following additional packages:

    cryptography

    cvxpy

    cython

    dask

    diffrax

    ecos

    faiss-cpu

    hdbscan

    highspy

    jax

    networkx

    numba

    numpy

    ortools

    pandas

    pot

    pulp

    pyomo

    python-sat

    scikit-learn

    scipy

    sympy

    torch

    YOUR TASK:
    Your objective is to define a class named 'Solver' in 'solver.py' with a method:
    """
    class Solver:
        def solve(self, problem, **kwargs) -> Any:
            """Your implementation goes here."""
            ...
    """
    IMPORTANT: Compilation time of your init function will not count towards your function's runtime.
    This 'solve' function will be the entrypoint called by the evaluation harness. Strive to align your class and method implementation as closely as possible with the desired performance criteria.
    For each instance, your function can run for at most 10x the baseline runtime for that instance. Strive to have your implementation run as fast as possible, while returning the same output as the baseline function (for the same given input). OpenEvolve may track multiple metrics (e.g., speed, accuracy, robustness); improvements to any target metric are valid provided correctness is preserved.

    OUTPUT FORMAT - READ CAREFULLY:
    You will receive the current Solver code and must respond with an IMPROVED complete version.
    
    Your response MUST follow this exact format:
    1. Brief analysis of optimization opportunities (1-3 sentences)
    2. Complete Python code in a ```python code block
    
    Example response format:
    The current implementation has redundant validation checks. I'll streamline the encryption path and use memoryview to reduce allocations.
    
    ```python
    from cryptography.hazmat.primitives.ciphers.aead import AESGCM
    
    class Solver:
        def solve(self, problem, **kwargs):
            # Your complete optimized implementation here
            pass
    ```
    
    CRITICAL RULES:
    - Always output the COMPLETE Solver class code, not partial edits
    - Code must be in a ```python block (starting with ```python on its own line)
    - Do NOT use command syntax (edit, eval, ls, etc.)
    - Do NOT output diffs or SEARCH/REPLACE blocks
    - The code must be runnable as-is
    
    OPTIMIZATION TIPS:
    - You have 8 CPU cores available for parallelization
    - Avoid if __name__ == "main" blocks (only solve() will be executed)
    - Compilation time of __init__() doesn't count towards runtime
    - After evaluation, you'll receive performance metrics vs baseline
    - Focus on speed while maintaining correctness

    GOALS:
    Your PRIMARY objective is to MAXIMIZE SPEEDUP (make your solver faster than baseline).
    - You succeed when speedup > 1.0x (faster than baseline)
    - You fail if speedup < 1.0x (slower than baseline) OR if any test fails
    - Higher speedup = better performance = higher score
    
    After each evaluation, focus on the speedup metric to guide your optimizations.

    Below you find the description of the task you will have to solve. Read it carefully and understand what the problem is and what your solver should do.
    <task/description.txt>

    Here is the baseline which you will be graded against. Your task is to write a function that produces the same output, in less time.
    <task.solve>

    This function will be used to check if your solution is valid for a given problem. If it returns False, it means the solution is invalid:
    <task.is_solution>
